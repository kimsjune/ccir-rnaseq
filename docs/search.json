[
  {
    "objectID": "week3/align.html#recap",
    "href": "week3/align.html#recap",
    "title": "A primer on RNA-seq analysis",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\nlast week, we created an index\nthis week, we will align our reads to the genome"
  },
  {
    "objectID": "week3/align.html#remeber-our-reads.fq.gz",
    "href": "week3/align.html#remeber-our-reads.fq.gz",
    "title": "A primer on RNA-seq analysis",
    "section": "Remeber our reads.fq.gz?",
    "text": "Remeber our reads.fq.gz?\nThese can be split across a few lanes, which should be pooled at this point.\n\n\nmkdir cat\ncat ./DKO_D1_L001/* ./DKO_D1_L002/* ./DKO_D1_L003/* ./DKO_D1_L004/* &gt; ./cat/DKO_D1.fastq.gz\n\n\n\n* is a wild card character; it matches any character at any length\n\ncat con-cat-enates our files together into DKO_D1.fastq.gz\n\nWe are running this from a log in node. With full sized files, you have to run these in a slurm job script\n\n\n\n# Not run\n\n#!/bin/bash\n#SBATCH --account=$SLURM_ACCOUNT\n#SBATCH --time=30:00\n#SBATCH --mem=4g\n#SBATCH --output=concat.out\n\ncat ./NTC_D1_L001/* ./NTC_D1_L002/* ./NTC_D1_L003/* ./NTC_D1_L004/* &gt; ./NTC_D1.fastq.gz\ncat ./NTC_D2_L001/* ./NTC_D2_L002/* ./NTC_D2_L003/* ./NTC_D2_L004/* &gt; ./NTC_D2.fastq.gz\ncat ./NTC_D3_L001/* ./NTC_D3_L002/* ./NTC_D3_L003/* ./NTC_D3_L004/* &gt; ./NTC_D3.fastq.gz\ncat ./NTC_G1_L001/* ./NTC_G1_L002/* ./NTC_G1_L003/* ./NTC_G1_L004/* &gt; ./NTC_G1.fastq.gz\ncat ./NTC_G2_L001/* ./NTC_G2_L002/* ./NTC_G2_L003/* ./NTC_G2_L004/* &gt; ./NTC_G2.fastq.gz\ncat ./NTC_G3_L001/* ./NTC_G3_L002/* ./NTC_G3_L003/* ./NTC_G3_L004/* &gt; ./NTC_G3.fastq.gz\ncat ./DKO_D1_L001/* ./DKO_D1_L002/* ./DKO_D1_L003/* ./DKO_D1_L004/* &gt; ./DKO_D1.fastq.gz\ncat ./DKO_D2_L001/* ./DKO_D2_L002/* ./DKO_D2_L003/* ./DKO_D2_L004/* &gt; ./DKO_D2.fastq.gz\ncat ./DKO_D3_L001/* ./DKO_D3_L002/* ./DKO_D3_L003/* ./DKO_D3_L004/* &gt; ./DKO_D3.fastq.gz\ncat ./DKO_G1_L001/* ./DKO_G1_L002/* ./DKO_G1_L003/* ./DKO_G1_L004/* &gt; ./DKO_G1.fastq.gz\ncat ./DKO_G2_L001/* ./DKO_G2_L002/* ./DKO_G2_L003/* ./DKO_G2_L004/* &gt; ./DKO_G2.fastq.gz\ncat ./DKO_G3_L001/* ./DKO_G3_L002/* ./DKO_G3_L003/* ./DKO_G3_L004/* &gt; ./DKO_G3.fastq.gz"
  },
  {
    "objectID": "week3/align.html#alignment-command",
    "href": "week3/align.html#alignment-command",
    "title": "A primer on RNA-seq analysis",
    "section": "Alignment command",
    "text": "Alignment command\n\n\nSTAR --runThreadN 4 --genomeDir /home/$USER/project/$USER/workshop/stargenome \\\n--readFilesIn /home/$USER/project/$USER/workshop/cat/DKO_D1.fastq.gz \\\n--outSAMtype BAM SortedByCoordinate --readFilesCommand zcat \\\n--outFileNamePrefix ./DKO_D1\n\n\n\n--runMode alignReads is omitted because it‚Äôs the default\n\n--runThreadN to use multiple cores\n\n--genomeDir is the directory that contains index files\n--outSAMtype BAM SortedByCoordinate outputs binary alignment file (.bam) that can be used downstream directly (instead of unsorted .sam)\n\n--readFilesCommand zcat: to use DKO_D1.fq.gz file directly"
  },
  {
    "objectID": "week3/align.html#alignment-job-script",
    "href": "week3/align.html#alignment-job-script",
    "title": "A primer on RNA-seq analysis",
    "section": "Alignment job script",
    "text": "Alignment job script\n\n\n#!/bin/bash\n#SBATCH --account=$SLURM_ACCOUNT\n#SBATCH --time=30:00\n#SBATCH --cpus-per-task=4\n#SBATCH --mem-per-cpu=8G\n#SBATCH --job-name=STAR_align\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n\nSTAR --runThreadN 4 --genomeDir /home/$USER/project/$USER/workshop/stargenome \\\n--readFilesIn /home/$USER/project/$USER/workshop/cat/DKO_D1.fastq.gz \\\n--outSAMtype BAM SortedByCoordinate --readFilesCommand zcat \\\n--outFileNamePrefix ./DKO_D1"
  },
  {
    "objectID": "week3/align.html#writing-and-saving-the-job-script",
    "href": "week3/align.html#writing-and-saving-the-job-script",
    "title": "A primer on RNA-seq analysis",
    "section": "Writing and saving the job script",
    "text": "Writing and saving the job script\nRecall we need to create a job script text file:\n\n\nnano align.sh\n\n\nctrl/cmd + x, y, enter to save\nHow do we check if STAR is loaded?\n\n\n\nSTAR --version\nmodule load star\n\n\n\n\nAaaaaaaaad, send it.\n\n\nsbatch align.sh\n\n\n\n\nCheck status\n\n\nsq"
  },
  {
    "objectID": "week3/align.html#summary",
    "href": "week3/align.html#summary",
    "title": "A primer on RNA-seq analysis",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\nnext week, we will use htseq to count how many reads are aligned to each gene"
  },
  {
    "objectID": "week1/intro.html#plan",
    "href": "week1/intro.html#plan",
    "title": "A primer on RNA-seq analysis",
    "section": "Plan",
    "text": "Plan\n\nIntro to RNA-seq analysis (today)\nRNA-seq pt 1\n\nindexing the genome\nread alignment\ncounting\n\nRNA-seq pt 2\n\ndifferential gene expression with edgeR\n\nRNA-seq pt 3\n\nvisualization"
  },
  {
    "objectID": "week1/intro.html#not-within-the-scope",
    "href": "week1/intro.html#not-within-the-scope",
    "title": "A primer on RNA-seq analysis",
    "section": "Not within the scope",
    "text": "Not within the scope\n\nwhen/why you should or shouldn‚Äôt do RNA-seq\n\nhow to purify RNA and send to sequencing (Toronto/McGill)\n\nbudget and design (might cover later)\n\ncore facility jobs\n\nRNA-seq library generation\n\nnon-core labs don‚Äôt typically make their own libraries\n\nIllumina sequencing technology (ex. bridge amplification)"
  },
  {
    "objectID": "week1/intro.html#overview",
    "href": "week1/intro.html#overview",
    "title": "A primer on RNA-seq analysis",
    "section": "Overview",
    "text": "Overview\n\nSample 1\n..ATGCTAGATCG‚Ä¶\n‚Ä¶GGTACTACT‚Ä¶\nSample 2\n..TGGTGTGCTG‚Ä¶\n‚Ä¶GCTAGCTGCA‚Ä¶\n\n\n\nGene\nSample 1\nSample 2\n‚Ä¶\n\n\n\n\nXYZ\n1234\n4312\n‚Ä¶\n\n\n‚Ä¶\n12\n44\n‚Ä¶"
  },
  {
    "objectID": "week1/intro.html#within-the-scope",
    "href": "week1/intro.html#within-the-scope",
    "title": "A primer on RNA-seq analysis",
    "section": "Within the scope",
    "text": "Within the scope\n\nYou have (or will have) some RNA-seq data to analyze\n\nie. you have (or will have) some samples.fq.gz files\n\nhow to go from here to PCA, Volcano plots, heatmaps, etc.? ü§îü§îü§î"
  },
  {
    "objectID": "week1/intro.html#two-phases-to-rna-seq-analysis",
    "href": "week1/intro.html#two-phases-to-rna-seq-analysis",
    "title": "A primer on RNA-seq analysis",
    "section": "Two phases to RNA-seq analysis",
    "text": "Two phases to RNA-seq analysis\n\n\n\n\n\n\n\n\nStep\nEnvironment\nRationale\n\n\n\n\nreads ‚û°Ô∏è count table\nHPC\n\nlarge file size\nhigh compute\nnon-interactive\n\n\n\ncount table ‚û°Ô∏è plots\nlocal\n\nsmall file size\nlow compute\ninteractive"
  },
  {
    "objectID": "week1/intro.html#working-on-a-high-performance-computing-cluster-hpc",
    "href": "week1/intro.html#working-on-a-high-performance-computing-cluster-hpc",
    "title": "A primer on RNA-seq analysis",
    "section": "Working on a high performance computing cluster (HPC)",
    "text": "Working on a high performance computing cluster (HPC)\n\nA big challenge to learning bioinformatics is the lack of graphical user interfaces (GUI).\n\n\nclick and drag\n\nInstead we have to use the command-line interface (CLI).\n\nwe have to type out what we want to do\n\nTo process reads into a count table, we need to use a HPC at a remote location."
  },
  {
    "objectID": "week1/intro.html#a-hpc-waterloo-graham",
    "href": "week1/intro.html#a-hpc-waterloo-graham",
    "title": "A primer on RNA-seq analysis",
    "section": "A HPC @ Waterloo: Graham",
    "text": "A HPC @ Waterloo: Graham\n\n\n\nit‚Äôs a big freaking computer\n\nwe can‚Äôt drive over to Waterloo every time\n\nWe can log in to graham from our computer using ssh"
  },
  {
    "objectID": "week1/intro.html#logging-into-graham-through-terminalcommand-prompt",
    "href": "week1/intro.html#logging-into-graham-through-terminalcommand-prompt",
    "title": "A primer on RNA-seq analysis",
    "section": "Logging into graham through Terminal/Command Prompt",
    "text": "Logging into graham through Terminal/Command Prompt\n\n\n\n\n\n\nWarning\n\n\nChange skim823 with your username!\n\n\n\n\n\n\nssh skim823@graham.computecanada.ca\n\n\n\n\nPrompts you with ‚Äúare you sure blah blah?‚Äù; type yes\nPrompts with your password; what you type is invisible for security\n\nPrompts you with MFA (annoying)"
  },
  {
    "objectID": "week1/intro.html#linux-terminal-basics",
    "href": "week1/intro.html#linux-terminal-basics",
    "title": "A primer on RNA-seq analysis",
    "section": "Linux terminal basics",
    "text": "Linux terminal basics\n\n\nBasic commands\n- pwd\n- ls\n- cd\n- cp\n- mv\n- mkdir\n- touch\n- rm\n- cat\n- head\n- tail\n\nTips\n- tab to ‚Äúauto‚Äù complete\n- ctrl/cmd + c to abort\n- ctrl/cmd + a to move the cursor to the beginning\n- ctrl/cmd + e to move the cursor to the end\n- ctrl/cmd + l to clear the screen"
  },
  {
    "objectID": "week1/intro.html#workflow-on-the-hpc-side",
    "href": "week1/intro.html#workflow-on-the-hpc-side",
    "title": "A primer on RNA-seq analysis",
    "section": "Workflow on the HPC side",
    "text": "Workflow on the HPC side\n\n\n\n\n\nwe need at least (and at most) three kinds of files:\n\ngenome.fa file\n\ngenes.gtf/.gff file\n\nreads.fq.gz files\n\n\nhopefully you got your reads.fq.gz from your sequencing core\n\ngenome is the reference build of your organism\n\nex. mm10, mm39, hg38\n\ngenes contains coordinates (among others) to all genes of that genome\n\nprotein-coding, non-coding, how well annotated?\n\nyou have to make sure your genome and genes match"
  },
  {
    "objectID": "week1/intro.html#downloading-these-files-to-our-hpc",
    "href": "week1/intro.html#downloading-these-files-to-our-hpc",
    "title": "A primer on RNA-seq analysis",
    "section": "Downloading these files to our HPC",
    "text": "Downloading these files to our HPC\nFirst, let‚Äôs create a new directory somewhere to save our files\n\n\n\n\n# go to your PROJECTS directory\nmkdir workshop\ncd workshop\n\n\n\nQuestion: what is your current directory?\nwget is a command we can use to download files over the internet\n\n\nwget https://hgdownload.cse.ucsc.edu/goldenpath/mm10/bigZips/mm10.fa.gz\nwget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M1/gencode.vM1.annotation.gtf.gz"
  },
  {
    "objectID": "week1/intro.html#downloading-these-files-to-our-hpc-1",
    "href": "week1/intro.html#downloading-these-files-to-our-hpc-1",
    "title": "A primer on RNA-seq analysis",
    "section": "Downloading these files to our HPC",
    "text": "Downloading these files to our HPC\nThese are our dummy reads (*.fq.gz):\n\n1% of all reads subsampled to reduce computational cost\n\nDKO for double KO cells\n\nD1 for DMSO treated replicate 1\n\nL00? for sequencing lanes\n\nsamples are sequenced across different ‚Äúlanes‚Äù on the sequencing machine\n\nto reduce lane bias? idk lol\n\n\nyou might get pooled reads or not depending on the core/machine setup\n\n\n\n\n\n\nwget https://github.com/kimsjune/ccir-bioinformatics2024/tree/main/week1/DKO_D1_L001/DKO_D1_L001.gz\nwget https://github.com/kimsjune/ccir-bioinformatics2024/tree/main/week1/DKO_D1_L002/DKO_D1_L002.gz\nwget https://github.com/kimsjune/ccir-bioinformatics2024/tree/main/week1/DKO_D1_L003/DKO_D1_L003.gz\nwget https://github.com/kimsjune/ccir-bioinformatics2024/tree/main/week1/DKO_D1_L004/DKO_D1_L004.gz"
  },
  {
    "objectID": "week1/intro.html#progress-so-far",
    "href": "week1/intro.html#progress-so-far",
    "title": "A primer on RNA-seq analysis",
    "section": "Progress so far",
    "text": "Progress so far\n\n\n\n\n\nwe got our reads.fq.gz, genome.fa, and genes.gtf\nnext week we will perform STAR index\n\nnecessary to align our reads to the genome"
  },
  {
    "objectID": "week1/intro.html#filesystem-and-policies",
    "href": "week1/intro.html#filesystem-and-policies",
    "title": "A primer on RNA-seq analysis",
    "section": "Filesystem and policies",
    "text": "Filesystem and policies\n\n\n\n\n\n\n\n\n\n\nFolder\nDefault quota\nBacked up?\nPurged?\nUse case\n\n\n\n\n/home/$USER\n50GB, 500k files\nY\nN\nsetup files\n\n\n/scratch/$USER\n20TB, 1M files\nN\nY, after 60d\nday-to-day (temp)\n\n\n/project/SLRUM_GROUP/$USER\n1TB, 500k files\nY\nN\nstatic data\n\n\n/home/$USER/nearline\n2TB, 5k files per group\nY\nN\nlong term storage\n\n\n\nDocumentation"
  },
  {
    "objectID": "week1/intro.html#resources",
    "href": "week1/intro.html#resources",
    "title": "A primer on RNA-seq analysis",
    "section": "Resources",
    "text": "Resources\n\n\nDigital Research Alliance of Canada\nDRAC wiki\nYoutube\ncommand-line stuff (bash)\nbasic commmands\nbasic commands\n(network stuff not relevant)\nRNA-seq\nGriffith lab\nData carpentry\nHarvard Core"
  },
  {
    "objectID": "week2/genome_index.html#recap",
    "href": "week2/genome_index.html#recap",
    "title": "A primer on RNA-seq analysis",
    "section": "Recap",
    "text": "Recap\n\n\n\n\n\nlast week, we downloaded all the necessary files\n\nwe will do our real first ‚Äúthing‚Äù‚Äîcreating an index"
  },
  {
    "objectID": "week2/genome_index.html#setup",
    "href": "week2/genome_index.html#setup",
    "title": "A primer on RNA-seq analysis",
    "section": "Setup",
    "text": "Setup\n\n\n# This is a comment; it does not get executed.\n# Log in to Graham\nssh skim823@graham.computecanada.ca\n\n\n\n# Remember where we downloaded our files?\ncd /home/$USER/project/$USER/workshop\npwd # Check where we are"
  },
  {
    "objectID": "week2/genome_index.html#unzipping",
    "href": "week2/genome_index.html#unzipping",
    "title": "A primer on RNA-seq analysis",
    "section": "Unzipping",
    "text": "Unzipping\nOur files are gzipped (compressed). So they have to be un-zipped with gzip.\n\n\ngzip --help\n\n\n\ngzip -c mm10.fa.gz\ngzip -c gencode.vM1.annotation.gtf.gz\n# Are the files there?\nls -l\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse --help or -h to see a help menu\n\nUse --version or -v to see if the program is even loaded"
  },
  {
    "objectID": "week2/genome_index.html#log-in-vs.-compute-node",
    "href": "week2/genome_index.html#log-in-vs.-compute-node",
    "title": "A primer on RNA-seq analysis",
    "section": "Log in vs.¬†compute node",
    "text": "Log in vs.¬†compute node\n\nWhen we ssh into graham, we are at a log in node\n\nTo run computationally heavy stuff like indexing a genome, we need to submit it as a job to a compute node\n\nor else it will take forever\n\nWhat about the gzip thing just now? It‚Äôs okay for a log in node\nWe need to write a text file describing our command, then submit it to slurm\n\nslurm is a program running on the HPC that manages our requests"
  },
  {
    "objectID": "week2/genome_index.html#a-slurm-job-script-header",
    "href": "week2/genome_index.html#a-slurm-job-script-header",
    "title": "A primer on RNA-seq analysis",
    "section": "A slurm job script header",
    "text": "A slurm job script header\n\n\n#!/bin/bash\n#SBATCH --account=$SLURM_ACCOUNT\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=12\n#SBATCH --mem-per-cpu=16G\n#SBATCH --job-name=STAR_index\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n\n\nThese lines outline the resources we are requesting:\n\nline 1: a ‚Äúshebang‚Äù line that indicates this is a bash script (mandatory)\n\nline 2: our group account name (mandatory)\n\nlines 3-5: we are asking for 12 CPUs, 16G RAM each for 1 hr total\n\nIf our job is not done within 1 hr, tough luck. It will just terminate.\n\nlines 6-8: many programs spit out some metadata about what it‚Äôs doing and if anything is going wrong into stdout and stderr, respectively. Custom names for these files."
  },
  {
    "objectID": "week2/genome_index.html#setting-our-slurm_account",
    "href": "week2/genome_index.html#setting-our-slurm_account",
    "title": "A primer on RNA-seq analysis",
    "section": "Setting our $SLURM_ACCOUNT",
    "text": "Setting our $SLURM_ACCOUNT\nLet‚Äôs set our $SLURM_ACCOUNT environment variable so that you can copy & paste code.\n\n\ncd /home/$USER/projects && ls\n\n\n\n# Append our new environment variable to our settings file\n# Don't hit enter before changing \"YOUR_GROUP_NAME\"\necho export SLURM_ACCOUNT=YOUR_GROUP_NAME &gt;&gt; ~/.bashrc\n\n\n\n# This settings file needs to be reloaded\nsource ~/.bashrc\n# Check if SLURM_ACCOUNT is now set\necho $SLURM_ACCOUNT"
  },
  {
    "objectID": "week2/genome_index.html#indexing-with-star",
    "href": "week2/genome_index.html#indexing-with-star",
    "title": "A primer on RNA-seq analysis",
    "section": "Indexing with STAR",
    "text": "Indexing with STAR\nSTAR is a fast splice-aware read alignment program. Designed specifically for mapping RNA-seq reads. Cited some cool 38k times since 2013.\nOther alternatives: HISAT2, Kallisto, Salmon\n\n\nSTAR --runThreadN 12 --runMode genomeGenerate --genomeDir /home/$USER/project/$USER/workshop/stargenome --genomeFastaFiles /home/$USER/project/$USER/workshop/mm10.fa  --sjdbGTFfile /home/$USER/project/$USER/workshop/gencode.vM1.annotation.gtf --sjdbOverhang 74\n\n\nSTAR manual\n\nrunThreadN: running 12 cores in parallel\n\nrunMode: creating an index\n\ngenomeDir: where the output (index) will go\n\ngenomeFastaFiles: path to our reference genome\n\nsjdbGTFfile: path to our gene annotation\n\nsjdbOverhang: read length - 1"
  },
  {
    "objectID": "week2/genome_index.html#full-job-script",
    "href": "week2/genome_index.html#full-job-script",
    "title": "A primer on RNA-seq analysis",
    "section": "Full job script",
    "text": "Full job script\n\n\n#!/bin/bash\n#SBATCH --account=$SLURM_ACCOUNT\n#SBATCH --time=1:00:00\n#SBATCH --cpus-per-task=12\n#SBATCH --mem-per-cpu=16G\n#SBATCH --job-name=STAR_index\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n\nSTAR --runThreadN 12 --runMode genomeGenerate \\\n--genomeDir \\\n/home/$USER/project/$USER/workshop/stargenome \\\n--genomeFastaFiles \\\n/home/$USER/project/$USER/workshop/mm10.fa \\\n--sjdbGTFfile \\\n/home/$USER/project/$USER/workshop/gencode.vM1.annotation.gtf \\\n--sjdbOverhang 74"
  },
  {
    "objectID": "week2/genome_index.html#create-and-save-a-job-script-text-file-with-nano",
    "href": "week2/genome_index.html#create-and-save-a-job-script-text-file-with-nano",
    "title": "A primer on RNA-seq analysis",
    "section": "Create and save a job script text file with nano",
    "text": "Create and save a job script text file with nano\nnano is a built-in text editor. Use the arrow keys to navigate.\n\n\nnano index.sh\n\n\nPaste in the full job script, save with ctrl/cmd + x, hit y to ‚ÄúSave modified buffer‚Äù, and hit enter to confirm ‚ÄúFile Name to Write‚Äù."
  },
  {
    "objectID": "week2/genome_index.html#loading-star-with-module",
    "href": "week2/genome_index.html#loading-star-with-module",
    "title": "A primer on RNA-seq analysis",
    "section": "Loading STAR with module",
    "text": "Loading STAR with module\nOne of the benefits of these HPCs is that popular programs are pre-installed. They just have to be loaded before we submit our job script.\n\n\n# Prompts you to select a version by pressing a number\nSTAR --version\n\n\n\n# or load it directly like this\nmodule load star\n\n\n\n\n\n\n\n\nTip\n\n\n\nSometimes our module environment has to be right to even load certain programs\n\nmodule spider PROGRAM will tell you how to load certain programs\n\nmodule list shows all active modules"
  },
  {
    "objectID": "week2/genome_index.html#finally-running-it",
    "href": "week2/genome_index.html#finally-running-it",
    "title": "A primer on RNA-seq analysis",
    "section": "Finally running it",
    "text": "Finally running it\n\n\n\n\n\n\nWarning\n\n\nAlways check if your program (as a module) is loaded by running PROGRAM --version / -v / -V. If it‚Äôs not, your job will fail.\n\n\n\n\n\nSTAR --version\n\n\n\nsbatch index.sh\n\n\nCheck the slurm job schedule for your account:\n\n\nsq"
  },
  {
    "objectID": "week2/genome_index.html#hpcs-vs-cloud",
    "href": "week2/genome_index.html#hpcs-vs-cloud",
    "title": "A primer on RNA-seq analysis",
    "section": "HPCs vs Cloud",
    "text": "HPCs vs Cloud\nThese HPCs are free like free beer, but you often have to wait for your stuff to run.\n\nWait time depends on usage (CPUs, time, memory). slurm keeps track of these.\n\nYou‚Äôd want to avoid wasteful usage. You can check efficiency by running seff JOBID.\n\nslurm documentation\n\nFrequently used commands: sbatch, sq, sacct -b, seff\n\nOn the other hand, cloud services (AWS, Azure, GCP) have no wait times, but functionally not free."
  }
]